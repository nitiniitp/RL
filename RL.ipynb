{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLRdee6I7tw3"
      },
      "source": [
        "\n",
        "In the following, we run Twin Delayed Deep Deterministic Policy Gradient (TD3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0lwnQCWhElA"
      },
      "source": [
        "# Neural Network Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDbjVC6oalVN"
      },
      "source": [
        "We define classes for the actor and critic networks. In TD3, we prepare two neural networks for the critic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQj4wEKr7jWv"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class Critic(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "        self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f3 = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "        self.f4 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f5 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f6 = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def predict(self, state, action):\n",
        "        x = self.f1(tf.concat([state, action], axis=1))\n",
        "        x = self.f2(x)\n",
        "        q1 = self.f3(x)\n",
        "\n",
        "        x2 = self.f4(tf.concat([state, action], axis=1))\n",
        "        x2 = self.f5(x2)\n",
        "        q2 = self.f6(x2)\n",
        "\n",
        "        return q1, q2\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        x = self.f1(tf.concat([state, action], axis=1))\n",
        "        x = self.f2(x)\n",
        "        q1 = self.f3(x)\n",
        "\n",
        "        return q1\n",
        "\n",
        "class Actor(tf.keras.Model):\n",
        "  def __init__(self, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.mu =  tf.keras.layers.Dense(action_dim, activation='tanh')\n",
        "\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def predict(self, state):\n",
        "    x = self.f1(state)\n",
        "    x = self.f2(x)\n",
        "    a = self.max_action * self.mu(x)\n",
        "\n",
        "    return a\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pFfw7tE7sBn"
      },
      "source": [
        "We then define the class for TD3. The update of the actor and critic is defined in \"def train()\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC1xWehEaip1"
      },
      "source": [
        "class TD3(object):\n",
        "    def __init__(\n",
        "            self,\n",
        "            state_dim,\n",
        "            action_dim,\n",
        "            max_action,\n",
        "            discount=0.99,\n",
        "            tau=0.005,\n",
        "            policy_noise=0.2,\n",
        "            noise_clip=0.5,\n",
        "            policy_freq=2\n",
        "    ):\n",
        "\n",
        "        self.actor = Actor(action_dim, max_action)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(3e-4)\n",
        "\n",
        "        self.critic = Critic()\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(3e-4)\n",
        "\n",
        "        self.actor.compile(optimizer=self.actor_optimizer)\n",
        "        self.critic.compile(optimizer=self.critic_optimizer)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.discount = discount\n",
        "        self.tau = tau\n",
        "        self.policy_noise = policy_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.policy_freq = policy_freq\n",
        "\n",
        "        self.total_it = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        return tf.squeeze(self.actor.predict(state), axis=1)\n",
        "\n",
        "    def update_actor_target_network(self):\n",
        "        actor_weights = self.actor.get_weights()\n",
        "        actor_target_weights = self.actor_target.get_weights()\n",
        "        for i in range(len(actor_weights)):\n",
        "            actor_target_weights[i] = self.tau * actor_weights[i] + (1 - self.tau) * actor_target_weights[i]\n",
        "\n",
        "        self.actor_target.set_weights(actor_target_weights)\n",
        "\n",
        "    def update_critic_target_network(self):\n",
        "        critic_weights = self.critic.get_weights()\n",
        "        critic_target_weights = self.critic_target.get_weights()\n",
        "        for i in range(len(critic_weights)):\n",
        "            critic_target_weights[i] = self.tau * critic_weights[i] + (1 - self.tau) * critic_target_weights[i]\n",
        "\n",
        "        self.critic_target.set_weights(critic_target_weights)\n",
        "\n",
        "    def train(self, replay_buffer, batch_size=100):\n",
        "        self.total_it += 1\n",
        "\n",
        "        # Sample replay buffer\n",
        "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "        with tf.GradientTape() as tape_critic:\n",
        "            target_actions = self.actor_target.predict(next_state)\n",
        "\n",
        "            noise = tf.clip_by_value(tf.random.normal(shape=[*np.shape(target_actions)], mean=0.0, stddev=self.policy_noise),\n",
        "                                     -self.noise_clip, self.noise_clip)\n",
        "\n",
        "            next_action = (tf.clip_by_value(target_actions + noise, -self.max_action, self.max_action))\n",
        "\n",
        "            target_Q1, target_Q2 = self.critic_target.predict(next_state, next_action)\n",
        "            target_Q = tf.math.minimum(target_Q1, target_Q2)\n",
        "            target_Q = reward + not_done * self.discount * target_Q\n",
        "\n",
        "            current_Q1, current_Q2 = self.critic.predict(state, action)\n",
        "\n",
        "            critic_loss = tf.keras.losses.MSE(current_Q1, tf.stop_gradient(target_Q)) \\\n",
        "                          + tf.keras.losses.MSE(current_Q2, tf.stop_gradient(target_Q))\n",
        "\n",
        "            grads_critic = tape_critic.gradient(critic_loss, self.critic.trainable_variables)\n",
        "\n",
        "        self.critic_optimizer.apply_gradients(zip(grads_critic, self.critic.trainable_variables))\n",
        "\n",
        "        if self.total_it % self.policy_freq == 0:\n",
        "            with tf.GradientTape() as tape_actor:\n",
        "                actor_loss = - tf.math.reduce_mean(self.critic.Q1(state, self.actor.predict(state)))\n",
        "\n",
        "\n",
        "            grads_actor = tape_actor.gradient(actor_loss, self.actor.trainable_variables)\n",
        "            self.actor_optimizer.apply_gradients(zip(grads_actor, self.actor.trainable_variables))\n",
        "\n",
        "            self.update_actor_target_network()\n",
        "            self.update_critic_target_network()\n",
        "\n",
        "\n",
        "\n",
        "    def save_model(self, iter, seed, env_name, foldername='./model/td3'  ):\n",
        "        try:\n",
        "            import pathlib\n",
        "            pathlib.Path(foldername).mkdir(parents=True, exist_ok=True)\n",
        "            self.actor.save_weights(\n",
        "                       foldername + '/td3_actor_'+ env_name +\n",
        "                       '_seed' + str(seed) + '_iter' + str(iter) + '.tf')\n",
        "\n",
        "            print('models is saved for iteration', iter )\n",
        "\n",
        "        except:\n",
        "            print(\"A result directory does not exist and cannot be created. The trial results are not saved\")\n",
        "\n",
        "    def load_model(self, iter, seed, env_name, foldername='model/td3'  ):\n",
        "        self.actor.load_weights(foldername + '/td3_actor_' + env_name +\n",
        "            '_seed' + str(seed) + '_iter' + str(iter) + '.tf')\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWAgIHhdhcYD"
      },
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm1Og8m4cGDA"
      },
      "source": [
        "The samples collected through trials and errors are stored in the replay buffer. \"def sample()\" is a function that randomly samples a batch of samples from the replay buffer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10_yuayicCbp"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "        self.state = np.zeros((max_size, state_dim))\n",
        "        self.action = np.zeros((max_size, action_dim))\n",
        "        self.next_state = np.zeros((max_size, state_dim))\n",
        "        self.reward = np.zeros((max_size, 1))\n",
        "        self.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "    def add(self, state, action, next_state, reward, done):\n",
        "        self.state[self.ptr] = state\n",
        "        self.action[self.ptr] = action\n",
        "        self.next_state[self.ptr] = next_state\n",
        "        self.reward[self.ptr] = reward\n",
        "        self.not_done[self.ptr] = 1. - done\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "        return (\n",
        "            tf.convert_to_tensor(self.state[ind], dtype=tf.float32),\n",
        "            tf.convert_to_tensor(self.action[ind], dtype=tf.float32),\n",
        "            tf.convert_to_tensor(self.next_state[ind], dtype=tf.float32),\n",
        "            tf.convert_to_tensor(self.reward[ind], dtype=tf.float32),\n",
        "            tf.convert_to_tensor(self.not_done[ind], dtype=tf.float32)\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6xvdeuChkTE"
      },
      "source": [
        "# Training and evaluating procedures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhStR6soa5dl"
      },
      "source": [
        "We define a function for evaluating the policy. When evaluating the trained policy, we evaluate the performance without the exploration noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLp8X2BDbBJh"
      },
      "source": [
        "def evaluate_greedy(env_test, agent, args, test_iter, test_n, state_dim):\n",
        "\n",
        "    state_test = env_test.reset()\n",
        "    return_epi_test = 0\n",
        "    for t_test in range(int(args['max_episode_len'])):\n",
        "        if args['render_env']:\n",
        "                env_test.render()\n",
        "\n",
        "        action_test = agent.select_action(np.reshape(state_test, (1, state_dim)))\n",
        "        state_test2, reward_test, terminal_test, info_test = env_test.step(action_test)\n",
        "        state_test = state_test2\n",
        "        return_epi_test = return_epi_test + reward_test\n",
        "        if terminal_test:\n",
        "            break\n",
        "\n",
        "    print('test_iter:{:d}, nn:{:d}, return_epi_test: {:d}'.format(int(test_iter), int(test_n),\n",
        "                                                                      int(return_epi_test)))\n",
        "\n",
        "    return return_epi_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJgOjkxObQbL"
      },
      "source": [
        "Below is the training procedure. We collect samples through trials and errors and store them in the replay buffer. The policy is trained once after every one time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-LnStctbHOC"
      },
      "source": [
        "def train(env, env_test, agent, args, index):\n",
        "\n",
        "    # Initialize replay memory\n",
        "    total_step_cnt = 0\n",
        "    epi_cnt = 0\n",
        "    test_iter = 0\n",
        "    return_test = np.zeros((np.ceil(int(args['total_step_num']) / int(args['eval_step_freq'])).astype('int') + 1))\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "\n",
        "    while total_step_cnt in range( int(args['total_step_num']) ):\n",
        "\n",
        "        state = env.reset()\n",
        "        ep_reward = 0\n",
        "        T_end = False\n",
        "\n",
        "        for t in range(int(args['max_episode_len'])):\n",
        "\n",
        "            # Select action randomly or according to policy\n",
        "            if total_step_cnt < int(args['start_timesteps']):\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.clip(\n",
        "                        agent.select_action(np.array(state))\n",
        "                        + np.random.normal(0, max_action * float(args['expl_noise']), size=action_dim),\n",
        "                        -max_action, max_action)\n",
        "\n",
        "            state2, reward, terminal, info = env.step(action)\n",
        "            terminal_bool = float(terminal) if t < int(args['max_episode_len']) else 0\n",
        "\n",
        "            # Store data in replay buffer\n",
        "            replay_buffer.add(state, action, state2, reward, terminal_bool)\n",
        "\n",
        "            # Train agent after collecting sufficient data\n",
        "            if total_step_cnt >= int(args['start_timesteps']):\n",
        "                for i in range(int(args['update_freq'])):\n",
        "                    agent.train(replay_buffer, int(args['batch_size']))\n",
        "\n",
        "            if t == int(args['max_episode_len']) - 1:\n",
        "                T_end = True\n",
        "\n",
        "            state = state2\n",
        "            ep_reward += reward\n",
        "            total_step_cnt += 1\n",
        "\n",
        "            # Evaluate the deterministic policy\n",
        "            if total_step_cnt >= test_iter * int(args['eval_step_freq']) or total_step_cnt == 1:\n",
        "                print('total_step_cnt', total_step_cnt)\n",
        "                print('evaluating the deterministic policy...')\n",
        "                for test_n in range(int(args['test_num'])):\n",
        "                    return_epi_test = evaluate_greedy(env_test, agent, args, test_iter, test_n, state_dim)\n",
        "\n",
        "                    # Store the average of returns over the test episodes\n",
        "                    return_test[test_iter] = return_test[test_iter] + return_epi_test / float(args['test_num'])\n",
        "\n",
        "                print('return_test[{:d}] {:d}'.format(int(test_iter), int(return_test[test_iter])))\n",
        "                test_iter += 1\n",
        "\n",
        "            if total_step_cnt % int(args['model_save_freq'])==0:\n",
        "                    agent.save_model(iter=test_iter, seed=int(index), env_name=args['env'])\n",
        "\n",
        "\n",
        "            if terminal or T_end:\n",
        "                epi_cnt += 1\n",
        "                print('| Reward: {:d} | Episode: {:d} | Total step num: {:d} |'.format(int(ep_reward), epi_cnt, total_step_cnt ))\n",
        "\n",
        "                break\n",
        "\n",
        "    return return_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxW9O6IUfPeZ"
      },
      "source": [
        "Main funciton. To manage the hyperparamters of TD3, we use argeparse.\n",
        "We use a task in OpenAI Gym (https://gym.openai.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSJU8BAebYYO",
        "outputId": "37252913-da1f-40a0-c5ce-33ee7e5b84ef"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# run parameters\n",
        "parser.add_argument('--env', help='choose the gym env- tested on {Pendulum-v0}')\n",
        "parser.add_argument('--env-id', type=int, default=0, help='choose the gym env- tested on {Pendulum-v0}')\n",
        "parser.add_argument('--random-seed', help='random seed for repeatability', default=1)\n",
        "parser.add_argument('--max-episodes', help='max num of episodes to do while training', default=1001) \n",
        "parser.add_argument('--max-episode-len', help='max length of 1 episode', default=1000)\n",
        "parser.add_argument('--trial-num', help='number of trials', default=3)\n",
        "parser.add_argument('--render-env', help='render the gym env', action='store_true')\n",
        "parser.add_argument('--total-step-num', help='total number of time steps', default=25000)\n",
        "parser.add_argument('--eval-step-freq', help='frequency of evaluating the policy', default=5000)\n",
        "parser.add_argument('--test-num', help='number of test episodes', default=10)\n",
        "\n",
        "parser.add_argument('--result-file', help='file name for storing results from multiple trials',\n",
        "                    default='./results/trials/td3/trials_td3_')\n",
        "parser.add_argument('--trial-idx', help='index of trials', default=0)\n",
        "parser.add_argument('--monitor-dir', help='directory for recording', default='results/video/td3')\n",
        "parser.add_argument('--model-save-freq', help='frequency of evaluating the policy', default=10000)\n",
        "parser.add_argument('--model-folder',  default='./model/td3')\n",
        "\n",
        "parser.add_argument(\"--start_timesteps\", default=1e4, type=int)  # How many time steps purely random policy is run for\n",
        "parser.add_argument(\"--expl_noise\", default=0.1, type=float)  # Std of Gaussian exploration noise\n",
        "parser.add_argument(\"--batch_size\", default=256, type=int)  # Batch size for both actor and critic\n",
        "parser.add_argument(\"--update_freq\", default=1, type=int)  # Number of policy updates\n",
        "\n",
        "parser.set_defaults(render_env=False)\n",
        "\n",
        "args_tmp, unknown = parser.parse_known_args()\n",
        "\n",
        "if args_tmp.env is None:\n",
        "    env_dict = {0 : \"Pendulum-v0\",\n",
        "    }\n",
        "    args_tmp.env = env_dict[args_tmp.env_id]\n",
        "args = vars(args_tmp)\n",
        "\n",
        "return_set=[]\n",
        "for ite in range(int(args['trial_num'])):\n",
        "    print('Trial Number:', ite)\n",
        "\n",
        "    index = int(ite) + int(args['trial_idx'])\n",
        "    env = gym.make(args['env'])\n",
        "\n",
        "    np.random.seed(index )\n",
        "    env.seed(index)\n",
        "\n",
        "    env_test = gym.make(args['env'])\n",
        "    env_test.seed(index)\n",
        "\n",
        "    print('action_space.shape', env.action_space.shape)\n",
        "    print('observation_space.shape', env.observation_space.shape)\n",
        "    action_bound = float(env.action_space.high[0])\n",
        "\n",
        "    assert (env.action_space.high[0] == -env.action_space.low[0])\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    agent = TD3(state_dim=state_dim, action_dim=action_dim, max_action=action_bound)\n",
        "\n",
        "    step_R_i = train(env, env_test, agent, args, index)\n",
        "    return_set.append(step_R_i)\n",
        "\n",
        "    result_path = \"./results/trials/td3\"\n",
        "    result_filename = args['result_file'] +  \\\n",
        "                      '_update_freq_' + str(int(args['update_freq'])) + '_' + args['env'] +  \\\n",
        "                      '_trial_idx_' + str(index) + '.txt'\n",
        "    try:\n",
        "        import pathlib\n",
        "        pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
        "        np.savetxt(result_filename, np.asarray(step_R_i))\n",
        "        print('The result of the trial no.' + str(index) + ' was saved.')\n",
        "    except:\n",
        "        print(\"A result directory does not exist and cannot be created. The trial results are not saved\")\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number: 0\n",
            "action_space.shape (1,)\n",
            "observation_space.shape (3,)\n",
            "total_step_cnt 1\n",
            "evaluating the deterministic policy...\n",
            "test_iter:0, nn:0, return_epi_test: -1749\n",
            "test_iter:0, nn:1, return_epi_test: -959\n",
            "test_iter:0, nn:2, return_epi_test: -1636\n",
            "test_iter:0, nn:3, return_epi_test: -1071\n",
            "test_iter:0, nn:4, return_epi_test: -1764\n",
            "test_iter:0, nn:5, return_epi_test: -1782\n",
            "test_iter:0, nn:6, return_epi_test: -965\n",
            "test_iter:0, nn:7, return_epi_test: -1770\n",
            "test_iter:0, nn:8, return_epi_test: -961\n",
            "test_iter:0, nn:9, return_epi_test: -1791\n",
            "return_test[0] -1445\n",
            "| Reward: -1586 | Episode: 1 | Total step num: 200 |\n",
            "| Reward: -1206 | Episode: 2 | Total step num: 400 |\n",
            "| Reward: -1534 | Episode: 3 | Total step num: 600 |\n",
            "| Reward: -779 | Episode: 4 | Total step num: 800 |\n",
            "| Reward: -1729 | Episode: 5 | Total step num: 1000 |\n",
            "| Reward: -1458 | Episode: 6 | Total step num: 1200 |\n",
            "| Reward: -742 | Episode: 7 | Total step num: 1400 |\n",
            "| Reward: -1612 | Episode: 8 | Total step num: 1600 |\n",
            "| Reward: -877 | Episode: 9 | Total step num: 1800 |\n",
            "| Reward: -1720 | Episode: 10 | Total step num: 2000 |\n",
            "| Reward: -1295 | Episode: 11 | Total step num: 2200 |\n",
            "| Reward: -921 | Episode: 12 | Total step num: 2400 |\n",
            "| Reward: -1668 | Episode: 13 | Total step num: 2600 |\n",
            "| Reward: -1066 | Episode: 14 | Total step num: 2800 |\n",
            "| Reward: -896 | Episode: 15 | Total step num: 3000 |\n",
            "| Reward: -1171 | Episode: 16 | Total step num: 3200 |\n",
            "| Reward: -879 | Episode: 17 | Total step num: 3400 |\n",
            "| Reward: -1069 | Episode: 18 | Total step num: 3600 |\n",
            "| Reward: -1435 | Episode: 19 | Total step num: 3800 |\n",
            "| Reward: -880 | Episode: 20 | Total step num: 4000 |\n",
            "| Reward: -877 | Episode: 21 | Total step num: 4200 |\n",
            "| Reward: -1766 | Episode: 22 | Total step num: 4400 |\n",
            "| Reward: -1530 | Episode: 23 | Total step num: 4600 |\n",
            "| Reward: -1551 | Episode: 24 | Total step num: 4800 |\n",
            "total_step_cnt 5000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:1, nn:0, return_epi_test: -1295\n",
            "test_iter:1, nn:1, return_epi_test: -1038\n",
            "test_iter:1, nn:2, return_epi_test: -1497\n",
            "test_iter:1, nn:3, return_epi_test: -1236\n",
            "test_iter:1, nn:4, return_epi_test: -956\n",
            "test_iter:1, nn:5, return_epi_test: -1176\n",
            "test_iter:1, nn:6, return_epi_test: -1090\n",
            "test_iter:1, nn:7, return_epi_test: -1066\n",
            "test_iter:1, nn:8, return_epi_test: -1647\n",
            "test_iter:1, nn:9, return_epi_test: -962\n",
            "return_test[1] -1196\n",
            "| Reward: -882 | Episode: 25 | Total step num: 5000 |\n",
            "| Reward: -1724 | Episode: 26 | Total step num: 5200 |\n",
            "| Reward: -1598 | Episode: 27 | Total step num: 5400 |\n",
            "| Reward: -1071 | Episode: 28 | Total step num: 5600 |\n",
            "| Reward: -1176 | Episode: 29 | Total step num: 5800 |\n",
            "| Reward: -1589 | Episode: 30 | Total step num: 6000 |\n",
            "| Reward: -1336 | Episode: 31 | Total step num: 6200 |\n",
            "| Reward: -1577 | Episode: 32 | Total step num: 6400 |\n",
            "| Reward: -868 | Episode: 33 | Total step num: 6600 |\n",
            "| Reward: -1242 | Episode: 34 | Total step num: 6800 |\n",
            "| Reward: -1416 | Episode: 35 | Total step num: 7000 |\n",
            "| Reward: -1558 | Episode: 36 | Total step num: 7200 |\n",
            "| Reward: -1701 | Episode: 37 | Total step num: 7400 |\n",
            "| Reward: -978 | Episode: 38 | Total step num: 7600 |\n",
            "| Reward: -1067 | Episode: 39 | Total step num: 7800 |\n",
            "| Reward: -1186 | Episode: 40 | Total step num: 8000 |\n",
            "| Reward: -1744 | Episode: 41 | Total step num: 8200 |\n",
            "| Reward: -1731 | Episode: 42 | Total step num: 8400 |\n",
            "| Reward: -1151 | Episode: 43 | Total step num: 8600 |\n",
            "| Reward: -937 | Episode: 44 | Total step num: 8800 |\n",
            "| Reward: -874 | Episode: 45 | Total step num: 9000 |\n",
            "| Reward: -1070 | Episode: 46 | Total step num: 9200 |\n",
            "| Reward: -1229 | Episode: 47 | Total step num: 9400 |\n",
            "| Reward: -991 | Episode: 48 | Total step num: 9600 |\n",
            "| Reward: -887 | Episode: 49 | Total step num: 9800 |\n",
            "total_step_cnt 10000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:2, nn:0, return_epi_test: -1109\n",
            "test_iter:2, nn:1, return_epi_test: -1909\n",
            "test_iter:2, nn:2, return_epi_test: -1860\n",
            "test_iter:2, nn:3, return_epi_test: -1660\n",
            "test_iter:2, nn:4, return_epi_test: -1132\n",
            "test_iter:2, nn:5, return_epi_test: -1704\n",
            "test_iter:2, nn:6, return_epi_test: -1599\n",
            "test_iter:2, nn:7, return_epi_test: -1174\n",
            "test_iter:2, nn:8, return_epi_test: -1080\n",
            "test_iter:2, nn:9, return_epi_test: -1901\n",
            "return_test[2] -1513\n",
            "models is saved for iteration 3\n",
            "| Reward: -865 | Episode: 50 | Total step num: 10000 |\n",
            "| Reward: -1850 | Episode: 51 | Total step num: 10200 |\n",
            "| Reward: -1467 | Episode: 52 | Total step num: 10400 |\n",
            "| Reward: -1268 | Episode: 53 | Total step num: 10600 |\n",
            "| Reward: -1808 | Episode: 54 | Total step num: 10800 |\n",
            "| Reward: -1657 | Episode: 55 | Total step num: 11000 |\n",
            "| Reward: -1516 | Episode: 56 | Total step num: 11200 |\n",
            "| Reward: -1444 | Episode: 57 | Total step num: 11400 |\n",
            "| Reward: -1522 | Episode: 58 | Total step num: 11600 |\n",
            "| Reward: -1468 | Episode: 59 | Total step num: 11800 |\n",
            "| Reward: -1496 | Episode: 60 | Total step num: 12000 |\n",
            "| Reward: -1429 | Episode: 61 | Total step num: 12200 |\n",
            "| Reward: -1467 | Episode: 62 | Total step num: 12400 |\n",
            "| Reward: -1422 | Episode: 63 | Total step num: 12600 |\n",
            "| Reward: -1277 | Episode: 64 | Total step num: 12800 |\n",
            "| Reward: -1258 | Episode: 65 | Total step num: 13000 |\n",
            "| Reward: -1155 | Episode: 66 | Total step num: 13200 |\n",
            "| Reward: -798 | Episode: 67 | Total step num: 13400 |\n",
            "| Reward: -1530 | Episode: 68 | Total step num: 13600 |\n",
            "| Reward: -1519 | Episode: 69 | Total step num: 13800 |\n",
            "| Reward: -957 | Episode: 70 | Total step num: 14000 |\n",
            "| Reward: -271 | Episode: 71 | Total step num: 14200 |\n",
            "| Reward: -404 | Episode: 72 | Total step num: 14400 |\n",
            "| Reward: -1513 | Episode: 73 | Total step num: 14600 |\n",
            "| Reward: -13 | Episode: 74 | Total step num: 14800 |\n",
            "total_step_cnt 15000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:3, nn:0, return_epi_test: -431\n",
            "test_iter:3, nn:1, return_epi_test: -979\n",
            "test_iter:3, nn:2, return_epi_test: -12\n",
            "test_iter:3, nn:3, return_epi_test: -274\n",
            "test_iter:3, nn:4, return_epi_test: -442\n",
            "test_iter:3, nn:5, return_epi_test: -1497\n",
            "test_iter:3, nn:6, return_epi_test: -1494\n",
            "test_iter:3, nn:7, return_epi_test: -13\n",
            "test_iter:3, nn:8, return_epi_test: -144\n",
            "test_iter:3, nn:9, return_epi_test: -268\n",
            "return_test[3] -555\n",
            "| Reward: -393 | Episode: 75 | Total step num: 15000 |\n",
            "| Reward: -7 | Episode: 76 | Total step num: 15200 |\n",
            "| Reward: -398 | Episode: 77 | Total step num: 15400 |\n",
            "| Reward: -130 | Episode: 78 | Total step num: 15600 |\n",
            "| Reward: -132 | Episode: 79 | Total step num: 15800 |\n",
            "| Reward: -132 | Episode: 80 | Total step num: 16000 |\n",
            "| Reward: -467 | Episode: 81 | Total step num: 16200 |\n",
            "| Reward: -3 | Episode: 82 | Total step num: 16400 |\n",
            "| Reward: -383 | Episode: 83 | Total step num: 16600 |\n",
            "| Reward: -131 | Episode: 84 | Total step num: 16800 |\n",
            "| Reward: -127 | Episode: 85 | Total step num: 17000 |\n",
            "| Reward: -127 | Episode: 86 | Total step num: 17200 |\n",
            "| Reward: -378 | Episode: 87 | Total step num: 17400 |\n",
            "| Reward: -120 | Episode: 88 | Total step num: 17600 |\n",
            "| Reward: -129 | Episode: 89 | Total step num: 17800 |\n",
            "| Reward: -124 | Episode: 90 | Total step num: 18000 |\n",
            "| Reward: -125 | Episode: 91 | Total step num: 18200 |\n",
            "| Reward: -124 | Episode: 92 | Total step num: 18400 |\n",
            "| Reward: -120 | Episode: 93 | Total step num: 18600 |\n",
            "| Reward: -127 | Episode: 94 | Total step num: 18800 |\n",
            "| Reward: -1 | Episode: 95 | Total step num: 19000 |\n",
            "| Reward: -271 | Episode: 96 | Total step num: 19200 |\n",
            "| Reward: -126 | Episode: 97 | Total step num: 19400 |\n",
            "| Reward: -119 | Episode: 98 | Total step num: 19600 |\n",
            "| Reward: -124 | Episode: 99 | Total step num: 19800 |\n",
            "total_step_cnt 20000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:4, nn:0, return_epi_test: -282\n",
            "test_iter:4, nn:1, return_epi_test: -354\n",
            "test_iter:4, nn:2, return_epi_test: -126\n",
            "test_iter:4, nn:3, return_epi_test: -1\n",
            "test_iter:4, nn:4, return_epi_test: -128\n",
            "test_iter:4, nn:5, return_epi_test: -124\n",
            "test_iter:4, nn:6, return_epi_test: -116\n",
            "test_iter:4, nn:7, return_epi_test: -123\n",
            "test_iter:4, nn:8, return_epi_test: -3\n",
            "test_iter:4, nn:9, return_epi_test: -1\n",
            "return_test[4] -126\n",
            "models is saved for iteration 5\n",
            "| Reward: -229 | Episode: 100 | Total step num: 20000 |\n",
            "| Reward: -122 | Episode: 101 | Total step num: 20200 |\n",
            "| Reward: -119 | Episode: 102 | Total step num: 20400 |\n",
            "| Reward: -238 | Episode: 103 | Total step num: 20600 |\n",
            "| Reward: -363 | Episode: 104 | Total step num: 20800 |\n",
            "| Reward: -361 | Episode: 105 | Total step num: 21000 |\n",
            "| Reward: -129 | Episode: 106 | Total step num: 21200 |\n",
            "| Reward: -128 | Episode: 107 | Total step num: 21400 |\n",
            "| Reward: -242 | Episode: 108 | Total step num: 21600 |\n",
            "| Reward: -120 | Episode: 109 | Total step num: 21800 |\n",
            "| Reward: -234 | Episode: 110 | Total step num: 22000 |\n",
            "| Reward: -128 | Episode: 111 | Total step num: 22200 |\n",
            "| Reward: -128 | Episode: 112 | Total step num: 22400 |\n",
            "| Reward: -240 | Episode: 113 | Total step num: 22600 |\n",
            "| Reward: -123 | Episode: 114 | Total step num: 22800 |\n",
            "| Reward: -120 | Episode: 115 | Total step num: 23000 |\n",
            "| Reward: -347 | Episode: 116 | Total step num: 23200 |\n",
            "| Reward: -256 | Episode: 117 | Total step num: 23400 |\n",
            "| Reward: -127 | Episode: 118 | Total step num: 23600 |\n",
            "| Reward: -120 | Episode: 119 | Total step num: 23800 |\n",
            "| Reward: -125 | Episode: 120 | Total step num: 24000 |\n",
            "| Reward: -248 | Episode: 121 | Total step num: 24200 |\n",
            "| Reward: -119 | Episode: 122 | Total step num: 24400 |\n",
            "| Reward: -120 | Episode: 123 | Total step num: 24600 |\n",
            "| Reward: -265 | Episode: 124 | Total step num: 24800 |\n",
            "total_step_cnt 25000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:5, nn:0, return_epi_test: -233\n",
            "test_iter:5, nn:1, return_epi_test: -124\n",
            "test_iter:5, nn:2, return_epi_test: -117\n",
            "test_iter:5, nn:3, return_epi_test: -120\n",
            "test_iter:5, nn:4, return_epi_test: -122\n",
            "test_iter:5, nn:5, return_epi_test: -235\n",
            "test_iter:5, nn:6, return_epi_test: -128\n",
            "test_iter:5, nn:7, return_epi_test: -124\n",
            "test_iter:5, nn:8, return_epi_test: -118\n",
            "test_iter:5, nn:9, return_epi_test: -116\n",
            "return_test[5] -144\n",
            "| Reward: -125 | Episode: 125 | Total step num: 25000 |\n",
            "The result of the trial no.0 was saved.\n",
            "Trial Number: 1\n",
            "action_space.shape (1,)\n",
            "observation_space.shape (3,)\n",
            "total_step_cnt 1\n",
            "evaluating the deterministic policy...\n",
            "test_iter:0, nn:0, return_epi_test: -1268\n",
            "test_iter:0, nn:1, return_epi_test: -1287\n",
            "test_iter:0, nn:2, return_epi_test: -1143\n",
            "test_iter:0, nn:3, return_epi_test: -1408\n",
            "test_iter:0, nn:4, return_epi_test: -1339\n",
            "test_iter:0, nn:5, return_epi_test: -1345\n",
            "test_iter:0, nn:6, return_epi_test: -1287\n",
            "test_iter:0, nn:7, return_epi_test: -1331\n",
            "test_iter:0, nn:8, return_epi_test: -1418\n",
            "test_iter:0, nn:9, return_epi_test: -1117\n",
            "return_test[0] -1294\n",
            "| Reward: -1498 | Episode: 1 | Total step num: 200 |\n",
            "| Reward: -1252 | Episode: 2 | Total step num: 400 |\n",
            "| Reward: -1358 | Episode: 3 | Total step num: 600 |\n",
            "| Reward: -1528 | Episode: 4 | Total step num: 800 |\n",
            "| Reward: -1200 | Episode: 5 | Total step num: 1000 |\n",
            "| Reward: -1511 | Episode: 6 | Total step num: 1200 |\n",
            "| Reward: -1372 | Episode: 7 | Total step num: 1400 |\n",
            "| Reward: -1788 | Episode: 8 | Total step num: 1600 |\n",
            "| Reward: -872 | Episode: 9 | Total step num: 1800 |\n",
            "| Reward: -1391 | Episode: 10 | Total step num: 2000 |\n",
            "| Reward: -901 | Episode: 11 | Total step num: 2200 |\n",
            "| Reward: -1439 | Episode: 12 | Total step num: 2400 |\n",
            "| Reward: -1629 | Episode: 13 | Total step num: 2600 |\n",
            "| Reward: -1315 | Episode: 14 | Total step num: 2800 |\n",
            "| Reward: -966 | Episode: 15 | Total step num: 3000 |\n",
            "| Reward: -1808 | Episode: 16 | Total step num: 3200 |\n",
            "| Reward: -1264 | Episode: 17 | Total step num: 3400 |\n",
            "| Reward: -911 | Episode: 18 | Total step num: 3600 |\n",
            "| Reward: -977 | Episode: 19 | Total step num: 3800 |\n",
            "| Reward: -1430 | Episode: 20 | Total step num: 4000 |\n",
            "| Reward: -1636 | Episode: 21 | Total step num: 4200 |\n",
            "| Reward: -1491 | Episode: 22 | Total step num: 4400 |\n",
            "| Reward: -856 | Episode: 23 | Total step num: 4600 |\n",
            "| Reward: -1692 | Episode: 24 | Total step num: 4800 |\n",
            "total_step_cnt 5000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:1, nn:0, return_epi_test: -1358\n",
            "test_iter:1, nn:1, return_epi_test: -1275\n",
            "test_iter:1, nn:2, return_epi_test: -1434\n",
            "test_iter:1, nn:3, return_epi_test: -1284\n",
            "test_iter:1, nn:4, return_epi_test: -1299\n",
            "test_iter:1, nn:5, return_epi_test: -1773\n",
            "test_iter:1, nn:6, return_epi_test: -1274\n",
            "test_iter:1, nn:7, return_epi_test: -1411\n",
            "test_iter:1, nn:8, return_epi_test: -1181\n",
            "test_iter:1, nn:9, return_epi_test: -1253\n",
            "return_test[1] -1354\n",
            "| Reward: -1155 | Episode: 25 | Total step num: 5000 |\n",
            "| Reward: -1365 | Episode: 26 | Total step num: 5200 |\n",
            "| Reward: -1197 | Episode: 27 | Total step num: 5400 |\n",
            "| Reward: -1285 | Episode: 28 | Total step num: 5600 |\n",
            "| Reward: -875 | Episode: 29 | Total step num: 5800 |\n",
            "| Reward: -1025 | Episode: 30 | Total step num: 6000 |\n",
            "| Reward: -890 | Episode: 31 | Total step num: 6200 |\n",
            "| Reward: -1288 | Episode: 32 | Total step num: 6400 |\n",
            "| Reward: -748 | Episode: 33 | Total step num: 6600 |\n",
            "| Reward: -984 | Episode: 34 | Total step num: 6800 |\n",
            "| Reward: -947 | Episode: 35 | Total step num: 7000 |\n",
            "| Reward: -1194 | Episode: 36 | Total step num: 7200 |\n",
            "| Reward: -1069 | Episode: 37 | Total step num: 7400 |\n",
            "| Reward: -1182 | Episode: 38 | Total step num: 7600 |\n",
            "| Reward: -1038 | Episode: 39 | Total step num: 7800 |\n",
            "| Reward: -989 | Episode: 40 | Total step num: 8000 |\n",
            "| Reward: -872 | Episode: 41 | Total step num: 8200 |\n",
            "| Reward: -1308 | Episode: 42 | Total step num: 8400 |\n",
            "| Reward: -1295 | Episode: 43 | Total step num: 8600 |\n",
            "| Reward: -1290 | Episode: 44 | Total step num: 8800 |\n",
            "| Reward: -1044 | Episode: 45 | Total step num: 9000 |\n",
            "| Reward: -1614 | Episode: 46 | Total step num: 9200 |\n",
            "| Reward: -632 | Episode: 47 | Total step num: 9400 |\n",
            "| Reward: -1653 | Episode: 48 | Total step num: 9600 |\n",
            "| Reward: -974 | Episode: 49 | Total step num: 9800 |\n",
            "total_step_cnt 10000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:2, nn:0, return_epi_test: -1249\n",
            "test_iter:2, nn:1, return_epi_test: -1339\n",
            "test_iter:2, nn:2, return_epi_test: -1306\n",
            "test_iter:2, nn:3, return_epi_test: -1420\n",
            "test_iter:2, nn:4, return_epi_test: -1415\n",
            "test_iter:2, nn:5, return_epi_test: -1288\n",
            "test_iter:2, nn:6, return_epi_test: -1273\n",
            "test_iter:2, nn:7, return_epi_test: -1347\n",
            "test_iter:2, nn:8, return_epi_test: -1482\n",
            "test_iter:2, nn:9, return_epi_test: -1392\n",
            "return_test[2] -1351\n",
            "models is saved for iteration 3\n",
            "| Reward: -1510 | Episode: 50 | Total step num: 10000 |\n",
            "| Reward: -1312 | Episode: 51 | Total step num: 10200 |\n",
            "| Reward: -1199 | Episode: 52 | Total step num: 10400 |\n",
            "| Reward: -1553 | Episode: 53 | Total step num: 10600 |\n",
            "| Reward: -1671 | Episode: 54 | Total step num: 10800 |\n",
            "| Reward: -1578 | Episode: 55 | Total step num: 11000 |\n",
            "| Reward: -1511 | Episode: 56 | Total step num: 11200 |\n",
            "| Reward: -1525 | Episode: 57 | Total step num: 11400 |\n",
            "| Reward: -1480 | Episode: 58 | Total step num: 11600 |\n",
            "| Reward: -1422 | Episode: 59 | Total step num: 11800 |\n",
            "| Reward: -1426 | Episode: 60 | Total step num: 12000 |\n",
            "| Reward: -1535 | Episode: 61 | Total step num: 12200 |\n",
            "| Reward: -1314 | Episode: 62 | Total step num: 12400 |\n",
            "| Reward: -1509 | Episode: 63 | Total step num: 12600 |\n",
            "| Reward: -1219 | Episode: 64 | Total step num: 12800 |\n",
            "| Reward: -1197 | Episode: 65 | Total step num: 13000 |\n",
            "| Reward: -1170 | Episode: 66 | Total step num: 13200 |\n",
            "| Reward: -1108 | Episode: 67 | Total step num: 13400 |\n",
            "| Reward: -1508 | Episode: 68 | Total step num: 13600 |\n",
            "| Reward: -1493 | Episode: 69 | Total step num: 13800 |\n",
            "| Reward: -1514 | Episode: 70 | Total step num: 14000 |\n",
            "| Reward: -269 | Episode: 71 | Total step num: 14200 |\n",
            "| Reward: -1521 | Episode: 72 | Total step num: 14400 |\n",
            "| Reward: -398 | Episode: 73 | Total step num: 14600 |\n",
            "| Reward: -394 | Episode: 74 | Total step num: 14800 |\n",
            "total_step_cnt 15000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:3, nn:0, return_epi_test: -138\n",
            "test_iter:3, nn:1, return_epi_test: -259\n",
            "test_iter:3, nn:2, return_epi_test: -3\n",
            "test_iter:3, nn:3, return_epi_test: -269\n",
            "test_iter:3, nn:4, return_epi_test: -129\n",
            "test_iter:3, nn:5, return_epi_test: -136\n",
            "test_iter:3, nn:6, return_epi_test: -268\n",
            "test_iter:3, nn:7, return_epi_test: -129\n",
            "test_iter:3, nn:8, return_epi_test: -269\n",
            "test_iter:3, nn:9, return_epi_test: -134\n",
            "return_test[3] -173\n",
            "| Reward: -270 | Episode: 75 | Total step num: 15000 |\n",
            "| Reward: -286 | Episode: 76 | Total step num: 15200 |\n",
            "| Reward: -1509 | Episode: 77 | Total step num: 15400 |\n",
            "| Reward: -128 | Episode: 78 | Total step num: 15600 |\n",
            "| Reward: -508 | Episode: 79 | Total step num: 15800 |\n",
            "| Reward: -251 | Episode: 80 | Total step num: 16000 |\n",
            "| Reward: -374 | Episode: 81 | Total step num: 16200 |\n",
            "| Reward: -1 | Episode: 82 | Total step num: 16400 |\n",
            "| Reward: -265 | Episode: 83 | Total step num: 16600 |\n",
            "| Reward: -355 | Episode: 84 | Total step num: 16800 |\n",
            "| Reward: -128 | Episode: 85 | Total step num: 17000 |\n",
            "| Reward: -121 | Episode: 86 | Total step num: 17200 |\n",
            "| Reward: -132 | Episode: 87 | Total step num: 17400 |\n",
            "| Reward: -121 | Episode: 88 | Total step num: 17600 |\n",
            "| Reward: -131 | Episode: 89 | Total step num: 17800 |\n",
            "| Reward: -128 | Episode: 90 | Total step num: 18000 |\n",
            "| Reward: -249 | Episode: 91 | Total step num: 18200 |\n",
            "| Reward: -127 | Episode: 92 | Total step num: 18400 |\n",
            "| Reward: -119 | Episode: 93 | Total step num: 18600 |\n",
            "| Reward: -127 | Episode: 94 | Total step num: 18800 |\n",
            "| Reward: -2 | Episode: 95 | Total step num: 19000 |\n",
            "| Reward: -119 | Episode: 96 | Total step num: 19200 |\n",
            "| Reward: -124 | Episode: 97 | Total step num: 19400 |\n",
            "| Reward: -124 | Episode: 98 | Total step num: 19600 |\n",
            "| Reward: -127 | Episode: 99 | Total step num: 19800 |\n",
            "total_step_cnt 20000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:4, nn:0, return_epi_test: -126\n",
            "test_iter:4, nn:1, return_epi_test: -239\n",
            "test_iter:4, nn:2, return_epi_test: -124\n",
            "test_iter:4, nn:3, return_epi_test: -114\n",
            "test_iter:4, nn:4, return_epi_test: -126\n",
            "test_iter:4, nn:5, return_epi_test: -268\n",
            "test_iter:4, nn:6, return_epi_test: 0\n",
            "test_iter:4, nn:7, return_epi_test: -223\n",
            "test_iter:4, nn:8, return_epi_test: -1\n",
            "test_iter:4, nn:9, return_epi_test: -236\n",
            "return_test[4] -146\n",
            "models is saved for iteration 5\n",
            "| Reward: -355 | Episode: 100 | Total step num: 20000 |\n",
            "| Reward: -239 | Episode: 101 | Total step num: 20200 |\n",
            "| Reward: -118 | Episode: 102 | Total step num: 20400 |\n",
            "| Reward: -326 | Episode: 103 | Total step num: 20600 |\n",
            "| Reward: -1 | Episode: 104 | Total step num: 20800 |\n",
            "| Reward: -115 | Episode: 105 | Total step num: 21000 |\n",
            "| Reward: -1 | Episode: 106 | Total step num: 21200 |\n",
            "| Reward: -3 | Episode: 107 | Total step num: 21400 |\n",
            "| Reward: -122 | Episode: 108 | Total step num: 21600 |\n",
            "| Reward: -231 | Episode: 109 | Total step num: 21800 |\n",
            "| Reward: -125 | Episode: 110 | Total step num: 22000 |\n",
            "| Reward: -126 | Episode: 111 | Total step num: 22200 |\n",
            "| Reward: -117 | Episode: 112 | Total step num: 22400 |\n",
            "| Reward: -337 | Episode: 113 | Total step num: 22600 |\n",
            "| Reward: -119 | Episode: 114 | Total step num: 22800 |\n",
            "| Reward: -1 | Episode: 115 | Total step num: 23000 |\n",
            "| Reward: -354 | Episode: 116 | Total step num: 23200 |\n",
            "| Reward: -232 | Episode: 117 | Total step num: 23400 |\n",
            "| Reward: -231 | Episode: 118 | Total step num: 23600 |\n",
            "| Reward: -2 | Episode: 119 | Total step num: 23800 |\n",
            "| Reward: -247 | Episode: 120 | Total step num: 24000 |\n",
            "| Reward: -117 | Episode: 121 | Total step num: 24200 |\n",
            "| Reward: -122 | Episode: 122 | Total step num: 24400 |\n",
            "| Reward: -233 | Episode: 123 | Total step num: 24600 |\n",
            "| Reward: -2 | Episode: 124 | Total step num: 24800 |\n",
            "total_step_cnt 25000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:5, nn:0, return_epi_test: -129\n",
            "test_iter:5, nn:1, return_epi_test: -1\n",
            "test_iter:5, nn:2, return_epi_test: -120\n",
            "test_iter:5, nn:3, return_epi_test: -116\n",
            "test_iter:5, nn:4, return_epi_test: -295\n",
            "test_iter:5, nn:5, return_epi_test: -118\n",
            "test_iter:5, nn:6, return_epi_test: -233\n",
            "test_iter:5, nn:7, return_epi_test: -123\n",
            "test_iter:5, nn:8, return_epi_test: -3\n",
            "test_iter:5, nn:9, return_epi_test: -125\n",
            "return_test[5] -126\n",
            "| Reward: -122 | Episode: 125 | Total step num: 25000 |\n",
            "The result of the trial no.1 was saved.\n",
            "Trial Number: 2\n",
            "action_space.shape (1,)\n",
            "observation_space.shape (3,)\n",
            "total_step_cnt 1\n",
            "evaluating the deterministic policy...\n",
            "test_iter:0, nn:0, return_epi_test: -1115\n",
            "test_iter:0, nn:1, return_epi_test: -1107\n",
            "test_iter:0, nn:2, return_epi_test: -1249\n",
            "test_iter:0, nn:3, return_epi_test: -1332\n",
            "test_iter:0, nn:4, return_epi_test: -1352\n",
            "test_iter:0, nn:5, return_epi_test: -1284\n",
            "test_iter:0, nn:6, return_epi_test: -1027\n",
            "test_iter:0, nn:7, return_epi_test: -1379\n",
            "test_iter:0, nn:8, return_epi_test: -1371\n",
            "test_iter:0, nn:9, return_epi_test: -1404\n",
            "return_test[0] -1262\n",
            "| Reward: -1556 | Episode: 1 | Total step num: 200 |\n",
            "| Reward: -948 | Episode: 2 | Total step num: 400 |\n",
            "| Reward: -1738 | Episode: 3 | Total step num: 600 |\n",
            "| Reward: -1341 | Episode: 4 | Total step num: 800 |\n",
            "| Reward: -826 | Episode: 5 | Total step num: 1000 |\n",
            "| Reward: -1571 | Episode: 6 | Total step num: 1200 |\n",
            "| Reward: -1233 | Episode: 7 | Total step num: 1400 |\n",
            "| Reward: -973 | Episode: 8 | Total step num: 1600 |\n",
            "| Reward: -1029 | Episode: 9 | Total step num: 1800 |\n",
            "| Reward: -965 | Episode: 10 | Total step num: 2000 |\n",
            "| Reward: -1060 | Episode: 11 | Total step num: 2200 |\n",
            "| Reward: -1798 | Episode: 12 | Total step num: 2400 |\n",
            "| Reward: -845 | Episode: 13 | Total step num: 2600 |\n",
            "| Reward: -1652 | Episode: 14 | Total step num: 2800 |\n",
            "| Reward: -1674 | Episode: 15 | Total step num: 3000 |\n",
            "| Reward: -1227 | Episode: 16 | Total step num: 3200 |\n",
            "| Reward: -1456 | Episode: 17 | Total step num: 3400 |\n",
            "| Reward: -909 | Episode: 18 | Total step num: 3600 |\n",
            "| Reward: -1435 | Episode: 19 | Total step num: 3800 |\n",
            "| Reward: -1012 | Episode: 20 | Total step num: 4000 |\n",
            "| Reward: -1463 | Episode: 21 | Total step num: 4200 |\n",
            "| Reward: -1844 | Episode: 22 | Total step num: 4400 |\n",
            "| Reward: -1516 | Episode: 23 | Total step num: 4600 |\n",
            "| Reward: -1361 | Episode: 24 | Total step num: 4800 |\n",
            "total_step_cnt 5000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:1, nn:0, return_epi_test: -1324\n",
            "test_iter:1, nn:1, return_epi_test: -1254\n",
            "test_iter:1, nn:2, return_epi_test: -1418\n",
            "test_iter:1, nn:3, return_epi_test: -1476\n",
            "test_iter:1, nn:4, return_epi_test: -1278\n",
            "test_iter:1, nn:5, return_epi_test: -1307\n",
            "test_iter:1, nn:6, return_epi_test: -1292\n",
            "test_iter:1, nn:7, return_epi_test: -1114\n",
            "test_iter:1, nn:8, return_epi_test: -1149\n",
            "test_iter:1, nn:9, return_epi_test: -1324\n",
            "return_test[1] -1294\n",
            "| Reward: -1188 | Episode: 25 | Total step num: 5000 |\n",
            "| Reward: -1314 | Episode: 26 | Total step num: 5200 |\n",
            "| Reward: -1715 | Episode: 27 | Total step num: 5400 |\n",
            "| Reward: -1246 | Episode: 28 | Total step num: 5600 |\n",
            "| Reward: -1619 | Episode: 29 | Total step num: 5800 |\n",
            "| Reward: -1321 | Episode: 30 | Total step num: 6000 |\n",
            "| Reward: -1073 | Episode: 31 | Total step num: 6200 |\n",
            "| Reward: -1058 | Episode: 32 | Total step num: 6400 |\n",
            "| Reward: -1724 | Episode: 33 | Total step num: 6600 |\n",
            "| Reward: -965 | Episode: 34 | Total step num: 6800 |\n",
            "| Reward: -1276 | Episode: 35 | Total step num: 7000 |\n",
            "| Reward: -915 | Episode: 36 | Total step num: 7200 |\n",
            "| Reward: -1342 | Episode: 37 | Total step num: 7400 |\n",
            "| Reward: -937 | Episode: 38 | Total step num: 7600 |\n",
            "| Reward: -1173 | Episode: 39 | Total step num: 7800 |\n",
            "| Reward: -1350 | Episode: 40 | Total step num: 8000 |\n",
            "| Reward: -1181 | Episode: 41 | Total step num: 8200 |\n",
            "| Reward: -741 | Episode: 42 | Total step num: 8400 |\n",
            "| Reward: -1079 | Episode: 43 | Total step num: 8600 |\n",
            "| Reward: -1446 | Episode: 44 | Total step num: 8800 |\n",
            "| Reward: -1657 | Episode: 45 | Total step num: 9000 |\n",
            "| Reward: -1004 | Episode: 46 | Total step num: 9200 |\n",
            "| Reward: -1332 | Episode: 47 | Total step num: 9400 |\n",
            "| Reward: -1175 | Episode: 48 | Total step num: 9600 |\n",
            "| Reward: -1387 | Episode: 49 | Total step num: 9800 |\n",
            "total_step_cnt 10000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:2, nn:0, return_epi_test: -1317\n",
            "test_iter:2, nn:1, return_epi_test: -1398\n",
            "test_iter:2, nn:2, return_epi_test: -1360\n",
            "test_iter:2, nn:3, return_epi_test: -1396\n",
            "test_iter:2, nn:4, return_epi_test: -1375\n",
            "test_iter:2, nn:5, return_epi_test: -1102\n",
            "test_iter:2, nn:6, return_epi_test: -1560\n",
            "test_iter:2, nn:7, return_epi_test: -1333\n",
            "test_iter:2, nn:8, return_epi_test: -1368\n",
            "test_iter:2, nn:9, return_epi_test: -1321\n",
            "return_test[2] -1353\n",
            "models is saved for iteration 3\n",
            "| Reward: -894 | Episode: 50 | Total step num: 10000 |\n",
            "| Reward: -1389 | Episode: 51 | Total step num: 10200 |\n",
            "| Reward: -1738 | Episode: 52 | Total step num: 10400 |\n",
            "| Reward: -1588 | Episode: 53 | Total step num: 10600 |\n",
            "| Reward: -1892 | Episode: 54 | Total step num: 10800 |\n",
            "| Reward: -1795 | Episode: 55 | Total step num: 11000 |\n",
            "| Reward: -1517 | Episode: 56 | Total step num: 11200 |\n",
            "| Reward: -1413 | Episode: 57 | Total step num: 11400 |\n",
            "| Reward: -1532 | Episode: 58 | Total step num: 11600 |\n",
            "| Reward: -1540 | Episode: 59 | Total step num: 11800 |\n",
            "| Reward: -1348 | Episode: 60 | Total step num: 12000 |\n",
            "| Reward: -1481 | Episode: 61 | Total step num: 12200 |\n",
            "| Reward: -1481 | Episode: 62 | Total step num: 12400 |\n",
            "| Reward: -1505 | Episode: 63 | Total step num: 12600 |\n",
            "| Reward: -1480 | Episode: 64 | Total step num: 12800 |\n",
            "| Reward: -1201 | Episode: 65 | Total step num: 13000 |\n",
            "| Reward: -1038 | Episode: 66 | Total step num: 13200 |\n",
            "| Reward: -1095 | Episode: 67 | Total step num: 13400 |\n",
            "| Reward: -1057 | Episode: 68 | Total step num: 13600 |\n",
            "| Reward: -1088 | Episode: 69 | Total step num: 13800 |\n",
            "| Reward: -1046 | Episode: 70 | Total step num: 14000 |\n",
            "| Reward: -395 | Episode: 71 | Total step num: 14200 |\n",
            "| Reward: -11 | Episode: 72 | Total step num: 14400 |\n",
            "| Reward: -1512 | Episode: 73 | Total step num: 14600 |\n",
            "| Reward: -273 | Episode: 74 | Total step num: 14800 |\n",
            "total_step_cnt 15000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:3, nn:0, return_epi_test: -262\n",
            "test_iter:3, nn:1, return_epi_test: -266\n",
            "test_iter:3, nn:2, return_epi_test: -1165\n",
            "test_iter:3, nn:3, return_epi_test: -263\n",
            "test_iter:3, nn:4, return_epi_test: -261\n",
            "test_iter:3, nn:5, return_epi_test: -262\n",
            "test_iter:3, nn:6, return_epi_test: -412\n",
            "test_iter:3, nn:7, return_epi_test: -131\n",
            "test_iter:3, nn:8, return_epi_test: -132\n",
            "test_iter:3, nn:9, return_epi_test: -1502\n",
            "return_test[3] -466\n",
            "| Reward: -138 | Episode: 75 | Total step num: 15000 |\n",
            "| Reward: -136 | Episode: 76 | Total step num: 15200 |\n",
            "| Reward: -136 | Episode: 77 | Total step num: 15400 |\n",
            "| Reward: -132 | Episode: 78 | Total step num: 15600 |\n",
            "| Reward: -539 | Episode: 79 | Total step num: 15800 |\n",
            "| Reward: -2 | Episode: 80 | Total step num: 16000 |\n",
            "| Reward: -124 | Episode: 81 | Total step num: 16200 |\n",
            "| Reward: -248 | Episode: 82 | Total step num: 16400 |\n",
            "| Reward: -271 | Episode: 83 | Total step num: 16600 |\n",
            "| Reward: 0 | Episode: 84 | Total step num: 16800 |\n",
            "| Reward: -371 | Episode: 85 | Total step num: 17000 |\n",
            "| Reward: -1 | Episode: 86 | Total step num: 17200 |\n",
            "| Reward: -254 | Episode: 87 | Total step num: 17400 |\n",
            "| Reward: -122 | Episode: 88 | Total step num: 17600 |\n",
            "| Reward: -133 | Episode: 89 | Total step num: 17800 |\n",
            "| Reward: -129 | Episode: 90 | Total step num: 18000 |\n",
            "| Reward: -125 | Episode: 91 | Total step num: 18200 |\n",
            "| Reward: -258 | Episode: 92 | Total step num: 18400 |\n",
            "| Reward: -123 | Episode: 93 | Total step num: 18600 |\n",
            "| Reward: -116 | Episode: 94 | Total step num: 18800 |\n",
            "| Reward: -241 | Episode: 95 | Total step num: 19000 |\n",
            "| Reward: -237 | Episode: 96 | Total step num: 19200 |\n",
            "| Reward: -129 | Episode: 97 | Total step num: 19400 |\n",
            "| Reward: -113 | Episode: 98 | Total step num: 19600 |\n",
            "| Reward: -117 | Episode: 99 | Total step num: 19800 |\n",
            "total_step_cnt 20000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:4, nn:0, return_epi_test: -5\n",
            "test_iter:4, nn:1, return_epi_test: -1\n",
            "test_iter:4, nn:2, return_epi_test: -115\n",
            "test_iter:4, nn:3, return_epi_test: -123\n",
            "test_iter:4, nn:4, return_epi_test: -306\n",
            "test_iter:4, nn:5, return_epi_test: -129\n",
            "test_iter:4, nn:6, return_epi_test: -125\n",
            "test_iter:4, nn:7, return_epi_test: -122\n",
            "test_iter:4, nn:8, return_epi_test: -244\n",
            "test_iter:4, nn:9, return_epi_test: -1\n",
            "return_test[4] -117\n",
            "models is saved for iteration 5\n",
            "| Reward: -126 | Episode: 100 | Total step num: 20000 |\n",
            "| Reward: -1 | Episode: 101 | Total step num: 20200 |\n",
            "| Reward: -269 | Episode: 102 | Total step num: 20400 |\n",
            "| Reward: -350 | Episode: 103 | Total step num: 20600 |\n",
            "| Reward: -223 | Episode: 104 | Total step num: 20800 |\n",
            "| Reward: -115 | Episode: 105 | Total step num: 21000 |\n",
            "| Reward: -127 | Episode: 106 | Total step num: 21200 |\n",
            "| Reward: -240 | Episode: 107 | Total step num: 21400 |\n",
            "| Reward: -114 | Episode: 108 | Total step num: 21600 |\n",
            "| Reward: -127 | Episode: 109 | Total step num: 21800 |\n",
            "| Reward: -355 | Episode: 110 | Total step num: 22000 |\n",
            "| Reward: -469 | Episode: 111 | Total step num: 22200 |\n",
            "| Reward: -125 | Episode: 112 | Total step num: 22400 |\n",
            "| Reward: -4 | Episode: 113 | Total step num: 22600 |\n",
            "| Reward: -118 | Episode: 114 | Total step num: 22800 |\n",
            "| Reward: -1192 | Episode: 115 | Total step num: 23000 |\n",
            "| Reward: -340 | Episode: 116 | Total step num: 23200 |\n",
            "| Reward: -1434 | Episode: 117 | Total step num: 23400 |\n",
            "| Reward: -1450 | Episode: 118 | Total step num: 23600 |\n",
            "| Reward: -268 | Episode: 119 | Total step num: 23800 |\n",
            "| Reward: -236 | Episode: 120 | Total step num: 24000 |\n",
            "| Reward: -119 | Episode: 121 | Total step num: 24200 |\n",
            "| Reward: -241 | Episode: 122 | Total step num: 24400 |\n",
            "| Reward: -123 | Episode: 123 | Total step num: 24600 |\n",
            "| Reward: -124 | Episode: 124 | Total step num: 24800 |\n",
            "total_step_cnt 25000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:5, nn:0, return_epi_test: -126\n",
            "test_iter:5, nn:1, return_epi_test: -238\n",
            "test_iter:5, nn:2, return_epi_test: -124\n",
            "test_iter:5, nn:3, return_epi_test: -328\n",
            "test_iter:5, nn:4, return_epi_test: -127\n",
            "test_iter:5, nn:5, return_epi_test: -122\n",
            "test_iter:5, nn:6, return_epi_test: -125\n",
            "test_iter:5, nn:7, return_epi_test: -122\n",
            "test_iter:5, nn:8, return_epi_test: -344\n",
            "test_iter:5, nn:9, return_epi_test: -4\n",
            "return_test[5] -166\n",
            "| Reward: -3 | Episode: 125 | Total step num: 25000 |\n",
            "The result of the trial no.2 was saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdSCbpbYhfME"
      },
      "source": [
        "We plot the results of the training using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "WjNBeCJPhdQY",
        "outputId": "c5d9a1fd-e632-49ba-d0c4-31f4dd788c2d"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import matplotlib\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "plt.rcParams['font.family'] = 'Times New Roman'\n",
        "\n",
        "figure(figsize=(7, 6))\n",
        "\n",
        "t = np.arange(0, int(args['total_step_num']) + 1, int(args['eval_step_freq'])) * 0.001\n",
        "\n",
        "mean = np.mean(np.asarray(return_set), axis=0)\n",
        "std = np.std(np.asarray(return_set), axis=0)\n",
        "color = 'b'\n",
        "label = 'TD3'\n",
        "plt.plot(t, mean, color, label=label)\n",
        "plt.fill(np.concatenate([t, t[::-1]]), np.concatenate([mean - 1.9600 * std,\n",
        "                                      (mean + 1.9600 * std)[::-1]]), alpha=.1, fc=color, ec='None')\n",
        "\n",
        "plt.xlabel('Time steps [x 1e3]', fontsize=14)\n",
        "plt.ylabel('Return', fontsize=14)\n",
        "plt.legend(loc='lower right', fontsize=14)\n",
        "\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}